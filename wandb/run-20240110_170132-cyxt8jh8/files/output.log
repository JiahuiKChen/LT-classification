Loading dataset from: /mnt/zhang-nas/tensorflow_datasets/downloads/manual/imagenet2012
{'coslr': True,
 'criterions': {'PerformanceLoss': {'def_file': './loss/SoftmaxLoss.py',
                                    'loss_params': {},
                                    'optim_params': None,
                                    'weight': 1.0}},
 'endlr': 0.0,
 'last': False,
 'memory': {'centroids': False, 'init_centroids': False},
 'model_dir': None,
 'networks': {'classifier': {'def_file': './models/DotProductClassifier.py',
                             'optim_params': {'lr': 0.2,
                                              'momentum': 0.9,
                                              'weight_decay': 0.0005},
                             'params': {'dataset': 'ImageNet_LT',
                                        'feat_dim': 2048,
                                        'log_dir': './logs/ImageNet_LT/models/resnext50_uniform_e90',
                                        'num_classes': 1000,
                                        'stage1_weights': False}},
              'feat_model': {'def_file': './models/ResNext50Feature.py',
                             'fix': False,
                             'optim_params': {'lr': 0.2,
                                              'momentum': 0.9,
                                              'weight_decay': 0.0005},
                             'params': {'dataset': 'ImageNet_LT',
                                        'dropout': None,
                                        'log_dir': './logs/ImageNet_LT/models/resnext50_uniform_e90',
                                        'stage1_weights': False,
                                        'use_fc': False,
                                        'use_selfatt': False}}},
 'shuffle': False,
 'training_opt': {'backbone': 'resnext50',
                  'batch_size': 512,
                  'dataset': 'ImageNet_LT',
                  'display_step': 10,
                  'feature_dim': 2048,
                  'log_dir': './logs/ImageNet_LT/models/resnext50_uniform_e90',
                  'log_root': '/logs/ImageNet_LT',
                  'num_classes': 1000,
                  'num_epochs': 150,
                  'num_workers': 4,
                  'open_threshold': 0.1,
                  'sampler': None,
                  'scheduler_params': {'gamma': 0.1, 'step_size': 30},
                  'stage': 'resnext50_uniform_e90',
                  'sub_dir': 'models',
                  'synth_data': True,
                  'synth_root': '/mnt/zhang-nas/jiahuic/synth_LT_data/test'}}
Loading data from ./data/ImageNet_LT/ImageNet_LT_train.txt
Use data transformation: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=warn)
    RandomHorizontalFlip(p=0.5)
    ColorJitter(brightness=(0.6, 1.4), contrast=(0.6, 1.4), saturation=(0.6, 1.4), hue=None)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
116106
No sampler.
Shuffle is True.
Loading data from ./data/ImageNet_LT/ImageNet_LT_train.txt
Use data transformation: Compose(
    Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
115846
No sampler.
Shuffle is True.
Loading data from ./data/ImageNet_LT/ImageNet_LT_val.txt
Use data transformation: Compose(
    Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
20000
No sampler.
Shuffle is True.
Loading data from ./data/ImageNet_LT/ImageNet_LT_test.txt
Use data transformation: Compose(
    Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
50000
No sampler.
Shuffle is True.
Using 3 GPUs.
Loading Dot Product Classifier.
Random initialized classifier weights.
Loading Scratch ResNext 50 Feature Model.
/mnt/zhang-nas/jiahuic/miniconda3/envs/diffusion/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
No Pretrained Weights For Feature Model.
Using steps for training.
Initializing model optimizer.
===> Using coslr eta_min=0.0
Loading Softmax Loss.
===> Saving cfg parameters to:  ./logs/ImageNet_LT/models/resnext50_uniform_e90/cfg.yaml
Phase: train
Do shuffle??? ---  False
/mnt/zhang-nas/jiahuic/miniconda3/envs/diffusion/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1695392026823/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)
  return F.conv2d(input, weight, bias, self.stride,
Epoch: [1/150] Step:     0  Minibatch_loss_performance: 7.039 Minibatch_accuracy_micro: 0.000
Epoch: [1/150] Step:    10  Minibatch_loss_performance: 8.507 Minibatch_accuracy_micro: 0.008
Epoch: [1/150] Step:    20  Minibatch_loss_performance: 8.103 Minibatch_accuracy_micro: 0.004
Epoch: [1/150] Step:    30  Minibatch_loss_performance: 7.390 Minibatch_accuracy_micro: 0.012
Epoch: [1/150] Step:    40  Minibatch_loss_performance: 6.897 Minibatch_accuracy_micro: 0.012
Epoch: [1/150] Step:    50  Minibatch_loss_performance: 6.779 Minibatch_accuracy_micro: 0.008
Epoch: [1/150] Step:    60  Minibatch_loss_performance: 6.673 Minibatch_accuracy_micro: 0.010
Epoch: [1/150] Step:    70  Minibatch_loss_performance: 6.645 Minibatch_accuracy_micro: 0.012
Epoch: [1/150] Step:    80  Minibatch_loss_performance: 6.496 Minibatch_accuracy_micro: 0.020
Epoch: [1/150] Step:    90  Minibatch_loss_performance: 6.557 Minibatch_accuracy_micro: 0.014
Epoch: [1/150] Step:   100  Minibatch_loss_performance: 6.445 Minibatch_accuracy_micro: 0.014
Epoch: [1/150] Step:   110  Minibatch_loss_performance: 6.408 Minibatch_accuracy_micro: 0.012
Epoch: [1/150] Step:   120  Minibatch_loss_performance: 6.512 Minibatch_accuracy_micro: 0.014
Epoch: [1/150] Step:   130  Minibatch_loss_performance: 6.449 Minibatch_accuracy_micro: 0.018
Epoch: [1/150] Step:   140  Minibatch_loss_performance: 6.503 Minibatch_accuracy_micro: 0.014
Epoch: [1/150] Step:   150  Minibatch_loss_performance: 6.503 Minibatch_accuracy_micro: 0.008
Epoch: [1/150] Step:   160  Minibatch_loss_performance: 6.390 Minibatch_accuracy_micro: 0.010
Epoch: [1/150] Step:   170  Minibatch_loss_performance: 6.475 Minibatch_accuracy_micro: 0.012
Epoch: [1/150] Step:   180  Minibatch_loss_performance: 6.427 Minibatch_accuracy_micro: 0.010
Epoch: [1/150] Step:   190  Minibatch_loss_performance: 6.423 Minibatch_accuracy_micro: 0.006
Epoch: [1/150] Step:   200  Minibatch_loss_performance: 6.467 Minibatch_accuracy_micro: 0.018
Epoch: [1/150] Step:   210  Minibatch_loss_performance: 6.384 Minibatch_accuracy_micro: 0.016
Epoch: [1/150] Step:   220  Minibatch_loss_performance: 6.444 Minibatch_accuracy_micro: 0.014
  0%|                                                    | 0/40 [00:00<?, ?it/s]
 Training acc Top1: 0.011
 Many_top1: 0.003 Median_top1: 0.000 Low_top1: 0.000















100%|███████████████████████████████████████████| 40/40 [01:05<00:00,  1.64s/it]
 Phase: val
 Evaluation_accuracy_micro_top1: 0.001
 Averaged F-measure: 0.000
 Many_shot_accuracy_top1: 0.004 Median_shot_accuracy_top1: 0.000 Low_shot_accuracy_top1: 0.000
===> Saving checkpoint
Epoch: [2/150] Step:     0  Minibatch_loss_performance: 6.464 Minibatch_accuracy_micro: 0.012
Epoch: [2/150] Step:    10  Minibatch_loss_performance: 6.475 Minibatch_accuracy_micro: 0.016
Epoch: [2/150] Step:    20  Minibatch_loss_performance: 6.473 Minibatch_accuracy_micro: 0.014
Epoch: [2/150] Step:    30  Minibatch_loss_performance: 6.372 Minibatch_accuracy_micro: 0.010
Epoch: [2/150] Step:    40  Minibatch_loss_performance: 6.416 Minibatch_accuracy_micro: 0.008
Epoch: [2/150] Step:    50  Minibatch_loss_performance: 6.348 Minibatch_accuracy_micro: 0.016
Epoch: [2/150] Step:    60  Minibatch_loss_performance: 6.406 Minibatch_accuracy_micro: 0.020
Epoch: [2/150] Step:    70  Minibatch_loss_performance: 6.341 Minibatch_accuracy_micro: 0.023
Epoch: [2/150] Step:    80  Minibatch_loss_performance: 6.410 Minibatch_accuracy_micro: 0.010
Epoch: [2/150] Step:    90  Minibatch_loss_performance: 6.392 Minibatch_accuracy_micro: 0.025
Epoch: [2/150] Step:   100  Minibatch_loss_performance: 6.384 Minibatch_accuracy_micro: 0.018
Epoch: [2/150] Step:   110  Minibatch_loss_performance: 6.297 Minibatch_accuracy_micro: 0.023
Epoch: [2/150] Step:   120  Minibatch_loss_performance: 6.198 Minibatch_accuracy_micro: 0.027
Epoch: [2/150] Step:   130  Minibatch_loss_performance: 6.316 Minibatch_accuracy_micro: 0.027
Epoch: [2/150] Step:   140  Minibatch_loss_performance: 6.249 Minibatch_accuracy_micro: 0.018
Epoch: [2/150] Step:   150  Minibatch_loss_performance: 6.327 Minibatch_accuracy_micro: 0.014
Epoch: [2/150] Step:   160  Minibatch_loss_performance: 6.249 Minibatch_accuracy_micro: 0.021
Epoch: [2/150] Step:   170  Minibatch_loss_performance: 6.264 Minibatch_accuracy_micro: 0.023
Epoch: [2/150] Step:   180  Minibatch_loss_performance: 6.340 Minibatch_accuracy_micro: 0.018
Epoch: [2/150] Step:   190  Minibatch_loss_performance: 6.231 Minibatch_accuracy_micro: 0.016
Epoch: [2/150] Step:   200  Minibatch_loss_performance: 6.270 Minibatch_accuracy_micro: 0.016
Epoch: [2/150] Step:   210  Minibatch_loss_performance: 6.190 Minibatch_accuracy_micro: 0.023
Epoch: [2/150] Step:   220  Minibatch_loss_performance: 6.242 Minibatch_accuracy_micro: 0.031
 Training acc Top1: 0.020
 Many_top1: 0.006 Median_top1: 0.000 Low_top1: 0.000
Phase: val
















100%|███████████████████████████████████████████| 40/40 [00:58<00:00,  1.47s/it]
 Phase: val
 Evaluation_accuracy_micro_top1: 0.003
 Averaged F-measure: 0.000
 Many_shot_accuracy_top1: 0.008 Median_shot_accuracy_top1: 0.000 Low_shot_accuracy_top1: 0.000
===> Saving checkpoint
Epoch: [3/150] Step:     0  Minibatch_loss_performance: 6.268 Minibatch_accuracy_micro: 0.016
Epoch: [3/150] Step:    10  Minibatch_loss_performance: 6.224 Minibatch_accuracy_micro: 0.021
Epoch: [3/150] Step:    20  Minibatch_loss_performance: 6.077 Minibatch_accuracy_micro: 0.027
Epoch: [3/150] Step:    30  Minibatch_loss_performance: 6.154 Minibatch_accuracy_micro: 0.031
Epoch: [3/150] Step:    40  Minibatch_loss_performance: 6.122 Minibatch_accuracy_micro: 0.027
Epoch: [3/150] Step:    50  Minibatch_loss_performance: 5.981 Minibatch_accuracy_micro: 0.043
Epoch: [3/150] Step:    60  Minibatch_loss_performance: 6.097 Minibatch_accuracy_micro: 0.035
Epoch: [3/150] Step:    70  Minibatch_loss_performance: 6.031 Minibatch_accuracy_micro: 0.035
Epoch: [3/150] Step:    80  Minibatch_loss_performance: 5.991 Minibatch_accuracy_micro: 0.043
Epoch: [3/150] Step:    90  Minibatch_loss_performance: 6.088 Minibatch_accuracy_micro: 0.025
Epoch: [3/150] Step:   100  Minibatch_loss_performance: 6.193 Minibatch_accuracy_micro: 0.018
Epoch: [3/150] Step:   110  Minibatch_loss_performance: 6.119 Minibatch_accuracy_micro: 0.029
Epoch: [3/150] Step:   120  Minibatch_loss_performance: 5.993 Minibatch_accuracy_micro: 0.021
Epoch: [3/150] Step:   130  Minibatch_loss_performance: 6.106 Minibatch_accuracy_micro: 0.029
Epoch: [3/150] Step:   140  Minibatch_loss_performance: 6.138 Minibatch_accuracy_micro: 0.025
Epoch: [3/150] Step:   150  Minibatch_loss_performance: 6.081 Minibatch_accuracy_micro: 0.033
Epoch: [3/150] Step:   160  Minibatch_loss_performance: 6.140 Minibatch_accuracy_micro: 0.020
Epoch: [3/150] Step:   170  Minibatch_loss_performance: 6.046 Minibatch_accuracy_micro: 0.025
Epoch: [3/150] Step:   180  Minibatch_loss_performance: 6.064 Minibatch_accuracy_micro: 0.031
Epoch: [3/150] Step:   190  Minibatch_loss_performance: 5.919 Minibatch_accuracy_micro: 0.029
