Loading dataset from: /mnt/zhang-nas/tensorflow_datasets/downloads/manual/imagenet2012
{'coslr': True,
 'criterions': {'PerformanceLoss': {'def_file': './loss/SoftmaxLoss.py',
                                    'loss_params': {},
                                    'optim_params': None,
                                    'weight': 1.0}},
 'endlr': 0.0,
 'last': False,
 'memory': {'centroids': False, 'init_centroids': False},
 'model_dir': None,
 'networks': {'classifier': {'def_file': './models/DotProductClassifier.py',
                             'optim_params': {'lr': 0.2,
                                              'momentum': 0.9,
                                              'weight_decay': 0.0005},
                             'params': {'dataset': 'ImageNet_LT',
                                        'feat_dim': 2048,
                                        'log_dir': './logs/ImageNet_LT/models/resnext50_uniform_e90',
                                        'num_classes': 1000,
                                        'stage1_weights': False}},
              'feat_model': {'def_file': './models/ResNext50Feature.py',
                             'fix': False,
                             'optim_params': {'lr': 0.2,
                                              'momentum': 0.9,
                                              'weight_decay': 0.0005},
                             'params': {'dataset': 'ImageNet_LT',
                                        'dropout': None,
                                        'log_dir': './logs/ImageNet_LT/models/resnext50_uniform_e90',
                                        'stage1_weights': False,
                                        'use_fc': False,
                                        'use_selfatt': False}}},
 'shuffle': False,
 'training_opt': {'backbone': 'resnext50',
                  'batch_size': 512,
                  'dataset': 'ImageNet_LT',
                  'display_step': 10,
                  'feature_dim': 2048,
                  'log_dir': './logs/ImageNet_LT/models/resnext50_uniform_e90',
                  'log_root': '/logs/ImageNet_LT',
                  'num_classes': 1000,
                  'num_epochs': 150,
                  'num_workers': 4,
                  'open_threshold': 0.1,
                  'sampler': None,
                  'scheduler_params': {'gamma': 0.1, 'step_size': 30},
                  'stage': 'resnext50_uniform_e90',
                  'sub_dir': 'models'}}
Loading data from ./data/ImageNet_LT/ImageNet_LT_train.txt
Use data transformation: Compose(
    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=warn)
    RandomHorizontalFlip(p=0.5)
    ColorJitter(brightness=(0.6, 1.4), contrast=(0.6, 1.4), saturation=(0.6, 1.4), hue=None)
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
115846
No sampler.
Shuffle is True.
Loading data from ./data/ImageNet_LT/ImageNet_LT_train.txt
Use data transformation: Compose(
    Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
115846
No sampler.
Shuffle is True.
Loading data from ./data/ImageNet_LT/ImageNet_LT_val.txt
Use data transformation: Compose(
    Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
20000
No sampler.
Shuffle is True.
Loading data from ./data/ImageNet_LT/ImageNet_LT_test.txt
Use data transformation: Compose(
    Resize(size=256, interpolation=bilinear, max_size=None, antialias=warn)
    CenterCrop(size=(224, 224))
    ToTensor()
    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
)
50000
No sampler.
Shuffle is True.
Using 4 GPUs.
Loading Dot Product Classifier.
Random initialized classifier weights.
Loading Scratch ResNext 50 Feature Model.
No Pretrained Weights For Feature Model.
Using steps for training.
Initializing model optimizer.
===> Using coslr eta_min=0.0
Loading Softmax Loss.
===> Saving cfg parameters to:  ./logs/ImageNet_LT/models/resnext50_uniform_e90/cfg.yaml
Phase: train
Do shuffle??? ---  False
/mnt/zhang-nas/jiahuic/miniconda3/envs/diffusion/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Traceback (most recent call last):
  File "/mnt/zhang-nas/jiahuic/LT-classification/main.py", line 145, in <module>
    training_model.train()
  File "/mnt/zhang-nas/jiahuic/LT-classification/run_networks.py", line 264, in train
    for step, (inputs, labels, indexes) in enumerate(self.data['train']):
  File "/mnt/zhang-nas/jiahuic/miniconda3/envs/diffusion/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/mnt/zhang-nas/jiahuic/miniconda3/envs/diffusion/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1345, in _next_data
    return self._process_data(data)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/zhang-nas/jiahuic/miniconda3/envs/diffusion/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1371, in _process_data
    data.reraise()
  File "/mnt/zhang-nas/jiahuic/miniconda3/envs/diffusion/lib/python3.11/site-packages/torch/_utils.py", line 694, in reraise
    raise exception
TypeError: Caught TypeError in DataLoader worker process 0.
Original Traceback (most recent call last):
  File "/mnt/zhang-nas/jiahuic/miniconda3/envs/diffusion/lib/python3.11/site-packages/torch/utils/data/_utils/worker.py", line 308, in _worker_loop
    data = fetcher.fetch(index)
           ^^^^^^^^^^^^^^^^^^^^
  File "/mnt/zhang-nas/jiahuic/miniconda3/envs/diffusion/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/zhang-nas/jiahuic/miniconda3/envs/diffusion/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 265, in default_collate
    return collate(batch, collate_fn_map=default_collate_fn_map)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/zhang-nas/jiahuic/miniconda3/envs/diffusion/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 142, in collate
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/zhang-nas/jiahuic/miniconda3/envs/diffusion/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 142, in <listcomp>
    return [collate(samples, collate_fn_map=collate_fn_map) for samples in transposed]  # Backwards compatibility.
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mnt/zhang-nas/jiahuic/miniconda3/envs/diffusion/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py", line 150, in collate
    raise TypeError(default_collate_err_msg_format.format(elem_type))
TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'PIL.Image.Image'>